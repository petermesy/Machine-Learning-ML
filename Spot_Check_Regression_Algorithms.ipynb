{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnwyaNq1iYD77dn2N46XiD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petermesy/Machine-Learning-ML/blob/main/Spot_Check_Regression_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzqcRAKuj9FY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Spot-Check Regression Algorithms"
      ],
      "metadata": {
        "id": "dKSawZ7ykEUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Linear Machine Learning Algorithm\n",
        "\n",
        " Linear Regression\n",
        "\n",
        "\n",
        " Linear regression assumes that the input variables have a Gaussian distribution. It is also\n",
        " assumed that input variables are relevant to the output variable and that they are not highly\n",
        " correlated with each other (a problem called collinearity)"
      ],
      "metadata": {
        "id": "ap8IHG-OkCoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "filname='/content/indian.csv'\n",
        "names=['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
        "dataframe=pd.read_csv(filname,names=names)\n",
        "dataframe=dataframe.apply(pd.to_numeric,errors='coerce')\n",
        "dataframe=dataframe.dropna()\n",
        "array=dataframe.values\n",
        "x=array[:,0:8]\n",
        "y=array[:,8]\n",
        "kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n"
      ],
      "metadata": {
        "id": "La186WfYkDP5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec-F0LbNlQfi",
        "outputId": "cec8c59f-b70e-49f0-ed9c-c413a5627a24"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.16320738575907065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LASSO Regression\n",
        "\n",
        "\n",
        " The Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modi cation\n",
        " of linear regression, like ridge regression, where the loss function is modi ed to minimize the\n",
        " complexity of the model measured as the sum absolute value of the coe cient values (also called\n",
        " the L1-norm). You can construct a LASSO model by using the Lasso class3."
      ],
      "metadata": {
        "id": "ae1ny_1cqG3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "model = Lasso()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg5DMxjOlpNm",
        "outputId": "9cf03921-6ce9-4345-fedd-8c318a22255e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.18012837070913562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ElasticNet Regression\n",
        "\n",
        "\n",
        " ElasticNet is a form of regularization regression that combines the properties of both Ridge\n",
        " Regression and LASSO regression. It seeks to minimize the complexity of the regression model\n",
        " (magnitude and number of regression coe cients) by penalizing the model using both the\n",
        " L2-norm (sum squared coe cient values) and the L1-norm (sum absolute coe cient values)."
      ],
      "metadata": {
        "id": "MSzFRWaGqs9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "model = ElasticNet()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vXlRLEzqblx",
        "outputId": "ea4cdf5d-96ba-4d57-f41f-1b75b12ace84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.17355666363344985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonlinear Machine Learning Algorithms\n",
        "\n",
        "\n",
        " This section provides examples of how to use three di erent nonlinear machine learning algorithms\n",
        " for regression in Python with scikit-learn."
      ],
      "metadata": {
        "id": "Id3fto3-rHvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors\n",
        "\n",
        "\n",
        " The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
        " training dataset for a new data instance"
      ],
      "metadata": {
        "id": "6M_xnJxLrIDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gf1RidcwrTJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "model = KNeighborsRegressor()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWoXIY_4q-IK",
        "outputId": "805b1c63-8b16-4ed9-c7ac-fa5d5374be63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.20382433356117566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classi cation and Regression Trees\n",
        "\n",
        "\n",
        " Decision trees or the Classi cation and Regression Trees (CART as they are known) use the train\n",
        "ing data to select the best points to split the data in order to minimize a cost metric. The default\n",
        " cost metric for regression decision trees is the mean squared error, speci ed in the criterion\n",
        " parameter."
      ],
      "metadata": {
        "id": "3MTwJN5ftwce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model = DecisionTreeRegressor()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpFV2qyWtnZz",
        "outputId": "e2dc2a66-838c-4bf3-a0f2-74e677562b2a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.3045625427204374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines\n",
        "\n",
        "\n",
        " Support Vector Machines (SVM) were developed for binary classi cation. The technique has\n",
        " been extended for the prediction real-valued problems called Support Vector Regression (SVR).\n",
        " Like the classi cation example, SVR is built upon the LIBSVM library"
      ],
      "metadata": {
        "id": "3P8HsKnyuHrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "model = SVR()\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
        "print(results.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNw1ZC92t_zH",
        "outputId": "2a7fb04f-410a-4e02-cafd-6729f8b9817f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.17461709362608288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y3H-sbdNufaT"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}