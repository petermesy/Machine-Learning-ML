{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMkiGOw5oIMGE3JPxvvxal",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petermesy/Machine-Learning-ML/blob/main/winEmbed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIkWEf99Y4g5",
        "outputId": "fdbd636f-ffd5-4cb9-a7e8-8e34ef006764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Processing lines: 169it [09:16,  3.29s/it]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model on GPU\n",
        "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\", device=device)\n",
        "\n",
        "# File paths\n",
        "input_path = \"/content/amharic_chunks.jsonl\"\n",
        "output_path = \"/content/amharic_chunks_with_embeddings.jsonl\"\n",
        "\n",
        "# Process file\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
        "     open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "\n",
        "    for line in tqdm(infile, desc=\"Processing lines\"):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        sentences = data.get(\"chunked_sentences\", [])\n",
        "        texts = [s.get(\"sentence\", \"\") for s in sentences]\n",
        "\n",
        "        # Batch encode with GPU\n",
        "        embeddings = model.encode(texts, batch_size=32, convert_to_numpy=False, device=device)\n",
        "\n",
        "        # Add embedding to each sentence\n",
        "        for s, emb in zip(sentences, embeddings):\n",
        "            s[\"embedding\"] = emb.tolist()\n",
        "\n",
        "        data[\"chunked_sentences\"] = sentences\n",
        "        outfile.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector DB with Qdrant"
      ],
      "metadata": {
        "id": "Wfop8Fh69UJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHjIycHlr3A",
        "outputId": "2e89dc02-2709-41a1-8b34-d84ec8fb0bf7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.11/dist-packages (1.14.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.0.2)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (5.29.4)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "import uuid\n",
        "\n",
        "# Use Cloud or local setup\n",
        "client = QdrantClient(\n",
        "    url=\"https://7a42a360-46c6-4155-a4a2-e358ec60b353.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.TK2U6DP4CyVj_W59YVOhbLDVEq07a2Y0b6Hm3PwcVhs\"\n",
        ")\n",
        "\n",
        "# Create collection (if not exists)\n",
        "client.recreate_collection(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FnLriwZlzWN",
        "outputId": "0c11e794-596d-4d98-80ed-eaa797aac402"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-1c645c6ce2b3>:12: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  client.recreate_collection(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client.models import PointStruct\n",
        "import uuid\n",
        "import json\n",
        "\n",
        "BATCH_SIZE = 500\n",
        "buffer = []\n",
        "\n",
        "with open(\"/content/amharic_chunks_with_embeddings.jsonl\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        for s in data[\"chunked_sentences\"]:\n",
        "            vector = s[\"embedding\"]\n",
        "            sentence_text = s[\"sentence\"]\n",
        "\n",
        "            point = PointStruct(\n",
        "                id=str(uuid.uuid4()),\n",
        "                vector=vector,\n",
        "                payload={\"text\": sentence_text}\n",
        "            )\n",
        "            buffer.append(point)\n",
        "\n",
        "            if len(buffer) >= BATCH_SIZE:\n",
        "                client.upsert(collection_name=\"amharic_sentences\", points=buffer)\n",
        "                buffer = []\n",
        "\n",
        "# Upload any remaining\n",
        "if buffer:\n",
        "    client.upsert(collection_name=\"amharic_sentences\", points=buffer)\n"
      ],
      "metadata": {
        "id": "MHoua8tmqhJS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"የኢትዮጵያ ፌደራላዊ መንግስት በሕግ ላይ በተመሰረተ መንግሥት መሆኑን ማረጋገጥና የፍትሕ ሥርዓት በተከታታይ ተቋማት መንደፍ እንደምን እንደሚከናወን መረዳት እፈልጋለሁ።\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(r.payload[\"text\"], \"→\", r.score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiYWKKcJxqIp",
        "outputId": "b7064ba3-9efa-40c3-a6ac-755cf0ad5854"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-ccb53f52fdd4>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ጥናት እና ምርምሮችን ሀ የአገሪቱን ህጎች የኢትዮጵያ ፌደራላዊ ዲሞክራሲያዊ ሪፐብሊክ ሕገ መንግስትን በተሟላ ሁኔታ ስራ ላይ ለማዋል ፣ የፍትህ እና የሕግ ሥርዓቱን ብቁና ዘመናዊ ለማድረግ ፣ የሕግ የበላይነትን ለማረጋገጥ እና በልማታዊና ዲሞክራሲያዊ አስተሳሰብና የእድገት አቅጣጫ ላይ የተመሰረተ የኢኮኖሚና ማሕበራዊ ልማትን ለማፋጠን በሚያስችሉበት ሁኔታ ለማሻሻል እንዲሁም አዳዲስ ሕጎችን ለማውጣት ፤ እና የፌዴራል ጠቅላይ አቃቤ ህግ 276 ለ የፍትህ አካላትን ዘመናዊ ፣ ተደራሽ ፣ ቀልጣፋና ውጤታማ በማድረግ በፍትህ ሥርዓቱ መልካም አስተዳደርን ለማስፈን ፤ እንዲረዳ ማካሄድ 2 . → 0.80455005\n",
            "ም በአማራ ብሔራዊ ክልላዊ መንግሥት የጠቅላይ ዓቃቤ ሕግ መስሪያ ቤት ማቋቋሚያና ስልጣንና ተግባራት መወሰኛ አዋጅ በክልሉ ውስጥ እስካሁን በተለያዩ ተቋማት አማካኝነት በተበታተነ መንገድ ሲከናወኑ የቆዩትን የዓቃቤ ሕግነት ተግባራት በማሰባሰብና ወደ አንድ አካል በማምጣት ወጥነት ያለው ፣ ለህብረተሰቡ ቀልጣፋና ውጤታማ አገልግሎት የሚሰጥ ፣ እንዲሁም የሕዝብንና የመንግስትን ጥቅም በተሟላ ሁኔታ የማስጠበቅ አቅም ያለው ፣ ራሱን የቻለና የተጠናከረ ሕግ አስከባሪ የዐቃቤ ሕግ ተቋም መመስረት አስፈላጊ ሆኖ በመገኘቱ ፤ ገጽ 2 የአማራ ብሔራዊ ክልላዊ መንግስት ዝክረ ሕግ ጋዜጣ ቁጥር 2 የካቲት 30 . → 0.798061\n",
            "ም በፌደራል መንግሥቱ ተዘጋጅቶ የጸደቀው የወንጀል ፍትሕ ፖሊሲ በክልሉ ውስጥ ተግባራዊ እንዲሆን ይሠራል ፣ ያስተባብራል ፣ ይከታተላል ፣ ያረጋግጣል ፤ 2 በክልሉ ውስጥ የወንጀል ፍትሕ መረጃዎችን ለመሰብሰብ ፣ ለማደራጀት ፣ ለመተንተንና ለማሰራጨት የሚያስችል ሥርዓት ይዘረጋል ፣ ያስፈጽማል ፤ 3 በሕግ ጉዳዮች ረገድ የክልሉ መንግሥት ዋና አማካሪ ሆኖ ይሠራል ፤ 4 የዓቃቤ ሕግ የውሳኔ አሰጣጥና የፍቅደ ሥልጣን አጠቃቀም ወጥነትን ለማረጋገጥ በፌደራሉ ፍትሕ ሚኒስቴር የጸደቀው መመሪያ በክልሉ ውስጥ ተግባራዊ መደረጉን ያረጋግጣል ፤ 5 ዓቃብያነ ሕግ የሚያሳልፏቸው ውሳኔዎች በሕግ መሠረት መሰጠታቸውን የሚያረጋግጥ የክትትልና የቁጥጥር ሥራዎች ማከናወኛ ክፍል ያደራጃል ፣ ጉድለቶችን በጥናት ላይ ተመሥርቶ ይለያል ፣ በግኝቱ መሠረትም የእርምት እርምጃዎችን ይወስዳል ወይም በሌሎች አካላት እንዲወሰዱ ያደርጋል ፤ 6 አግባብ ካላቸው አካላት ጋር በመተባበር በትርፍ ላይ ያልተመሠረተ ዓላማ ይዘው የተቋቋሙና በክልሉ ውስጥ የሚንቀሳቀሱ ማኅበራትንና መንግሥታዊ ያልሆኑ ድርጅቶችን ሕጋዊነት በማረጋገጥ ይመዘግባል ፣ ጉዳዩ ከሚመለከታቸው አካላት ጋር በመተባበር ይቆጣጠራል ፣ ይሰርዛል ፤ 1 ገጽ 73 የአማራ ብሔራዊ ክልላዊ መንግስት ዝክረ ሕግ ጋዜጣ ቁጥር 18 ጥር 15 . → 0.79571545\n",
            "በፌደራል መንግሥቱ ተዘጋጅቶ የፀደቀውን የወንጀል ፍትህ ፖሊሲ በክልሉ ውስጥ ተግባራዊ እንዲሆን ይሰራል ፣ ያስተባብራል ፣ ይከታተላል ፣ ያረጋግጣል ፤ 2 . → 0.77666676\n",
            "የኢትዮጵያ የደረጃዎች ኤጀንሲ ከዚህ በኋላ ኤጀንሲ እየተባለ የሚጠራ የሕግ ሰውነት ያለው ራሱን የቻለ የፌዴራል መንግሥት መሥሪያ ቤት ሆኖ በዚህ ደንብ ተቋቁሟል ። → 0.7761396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"የኢትዮጵያ ፌደራል አካላት እና ክልሎች መካከል የሥልጣን መመደብ እንዴት ነው? የህግ ትዳር እና የፖሊሲ ተግባራት አንፃፃፍ ማን ይመራዋል?\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(\"Score:\", r.score)\n",
        "    print(\"Match:\", r.payload[\"text\"])\n",
        "    print(\"-----------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTr1w4RSycKY",
        "outputId": "c9367fc6-2e87-47e1-d296-0803e4137cae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ecf0ec653c2d>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.8323995\n",
            "Match: የክልሎች ሥልጣንና ሃላፊነት ክልሎች የሚከተሉት ሥልጣንና ሀላፊነቶች ይኖራቸዋል .\n",
            "-----------\n",
            "Score: 0.8232411\n",
            "Match: የክልሉ መንግስት መስተዳድር ምክር ቤት ሥልጣንና ተግባር 1 የክልሉ መንግስት መስተዳድር ምክር ቤት በክልሉ ህገመንግሥት አንቀጽ 5 ሥር የተዘረዘሩት ሥልጣንና ተግባራት የሚኖሩት ሆኖ የአሠራር ሥርዓቱ መስተዳድር ምክር ቤቱ በሚያወጣው ደንብ የሚወሰን ይሆናል 2 የክልሉ መንግስት መስተዳድር ምክር ቤት ተጠሪነት ለጨፌና ለክልሉ ፕሬዝዳንት ይሆናል 3 በማንኛውም ደረጃ የሚገኙ የመስተዳድሩ ምክር ቤት አባላት በአባልነታቸው ለሚሰጡት ውሳኔ የጋራ ተጠያቂነት ይኖራቸዋል 7 .\n",
            "-----------\n",
            "Score: 0.8213489\n",
            "Match: በኢትዮጵያ ፌዴራላዊ ዲሞክራሲያዊ ሪፐብሊክ አስፈፃሚ አካላት ሥልጣንና ተግባር ለመወሰን በወጣው አዋጅ ቁጥር 916 2008 እና በሌሎች ሕጎች ለፍትሕ ሚኒስቴር ተሰጥተው የነበሩ ሥልጣንና ተግባራት በዚህ አዋጅ መሠረት ለፌዴራል ጠቅላይ ዓቃቤ ሕግ ተላልፈዋል ።\n",
            "-----------\n",
            "Score: 0.8176497\n",
            "Match: ስለዚህ የፌደራል የመንግሥት አስተዳደር ዋና መለያ ባህሪ ሥልጣን በፌደራል መንግሥቱና በክልሎች መካከል የተከፋፈለ በመሆኑ በዚህ የስልጣን ክፍፍል መሰረት ሁለቱም አካላት ግብር የሚሰበስቡባቸው አካባቢዎች ተለይተው ተዘርዝረዋል ።\n",
            "-----------\n",
            "Score: 0.8175594\n",
            "Match: በመተዳደሪያ ደንቡ በሚወሰነው መሠረት የፖለቲካ ሥራውን የሚመሩ ፣ ውሳኔ የሚሰጡ እና የሚያስፈጽሙ የአመራር አካላት ይኖሩታል ።\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"የማህበረሰብ ተቋማት እና የህገ መንግስት መንፈስ መካከል ያለው ግንኙነት በማንኛውም ዓይነት ፕሮግራሞች ወይም ተግባራዊ አሰራሮች ውስጥ እንዴት ነው የተሳተፈው?\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(\"Score:\", r.score)\n",
        "    print(\"Match:\", r.payload[\"text\"])\n",
        "    print(\"-----------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHNxDE17y3oQ",
        "outputId": "b7167c77-d427-4dbb-c6bd-f31b9454118b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ac455e50fbcc>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.7853646\n",
            "Match: ባሇዴርሻ አካሊት ባሇዴርሻ አካሊት በሚያከናውኗቸው ስራዎች ውስጥ የሕገመንግሥትና ፋዯራሉዜም የአስተምህሮ ተግባራትን የማካተት ፣ ሇአመራሮችና ሰራተኞች የሕገመንግሥትና ፋዯራሉዜም ግንዚቤያቸውን የማሳዯግ ፣ የተሇያዩ የአሰራር ሥርዓቶችን በመ ርጋት የህብረተሰቡን ግንዚቤ በማጎሌበት ከማዕከለ ጋር ተባብሮ የመስራት ኃሊፉነት አሇባቸው 15 .\n",
            "-----------\n",
            "Score: 0.7687364\n",
            "Match: ከመንግስት የኢኮኖሚ ነከ ተቋማት ጋር ስለሚኖር ግንኙነት 1 .\n",
            "-----------\n",
            "Score: 0.7580621\n",
            "Match: በፌደራል መንግስት ፕሮጀክቶች ላይ የፌደራል መንግስት እና ክልሎች ግንኙነት የፌደራል መንግስት እና ክልሎች በፌደራል መንግስት ፕሮጀክቶች ላይ በሚከተሉት ጉዳዮች ላይ በትብብር ይሰራሉ ፩ ለፕሮጀክቶች መሬት ማቅረብ ፤ ፪ በፕሮጀክቶች ምክንያት ለሚነሱ ማህበረሰቦች በአግባቡና በህግ መሰረት ካሣ መከፈሉን ማረጋገጥ ፤ ፫ በፕሮጀክቶች ዝግጅት እና ትግበራ ሂደት የአካባቢው ማህበረሰብ ተሳታፊ መሆኑን ማረጋገጥ ፤ ፬ ፕሮጀክቶች ትግበራ ሂደት በፕሮጀክቱና በህብረተሰቡ መካካል የሚነሱ አለመግባባቶችን መፍታት ፤ ፭ በፕሮጀክት አፈፃፀም መረጃ መሰረት በፕሮጀክቱ አካባቢ ለሚኖሩ ማህበረሰቦች ስለፕሮጀክቶቹ ግንዛቤ ማስጨበጥ 3 .\n",
            "-----------\n",
            "Score: 0.7515875\n",
            "Match: መንግሥት ሇሀገሪቱ የኢኮኖሚ እና ማኅበራዊ ዔዴገት ወሳኝ ሚና የሚኖራቸውን የጂኦተርማሌ ሥራዎች በራሱ ወይም ከላልች ባሇሀብቶች ጋር በቅንጅት ሉያካሂዴ ይችሊሌ 2 .\n",
            "-----------\n",
            "Score: 0.75008655\n",
            "Match: በዚህም መሠረት በመንግሥትና ማሕበረሰብ እንዲሁም በፖለቲካ ልሂቃን መካከል በዴሞክራሲያዊ አግባብ የሚፈጠሩ ስምምነቶችና የመንግሥት ቅቡልነት ለልማት መሠረት ለሆኑት ሰላም መስፈን ፣ የማሕበረሰብ ትጋት ፣ በመንግሥት ተቋማት በተለይም የሲቪል ሰርቪስ ውስጥ ለሚፈጠር የአገልጋይነት መንፈስና ትጋት ትልቅ አቅም ይፈጥራሉ ።\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"የህግ ማሻሻያ ሂደት ምን ያመለክታል?\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(\"Score:\", r.score)\n",
        "    print(\"Match:\", r.payload[\"text\"])\n",
        "    print(\"-----------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqchjvyq3DXC",
        "outputId": "1e6070a3-8cda-4bec-cb9d-ad2312f510f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-6e9a53788e64>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.80634755\n",
            "Match: ም በእርቅ ስምምነት ተከፋፍለዉ ይህን ስምምነታቸዉን ደግሞ ለፌዴራል የመጀመሪያ ደረጃ ፍ ቤት አቅርበዉ ፍ ቤቱም ስምምነቱን በሕጉ አግባብ በማየት ማጽደቁን የሥር ዉሳኔ የሚያመለክተዉ ጉዳይ ነዉ እንግዲህ የአሁን ተጠሪ ባቀረበዉ የፍርድ መቃወም አቤቱታ አመልካችና ሌሎች ወራሾች በስምምነት የመንግስትን ይዞታ ስለተከፋፈሉ በነባር ይዞታ አግባብ እና በሊዝ አዋጁ መሰረት እንዲፈጸም ዳኝነት መጠየቁን ክርክሩ ያሳያል አመልካች አጥብቀዉ የሚከራከሩት በዚህ ጉዳይ የሊዝ አዋጁ ተፈጻሚነት የለዉም የሚል ነዉ ስለሆነም 321 በዚህ ጉዳይ መታየት ያለበት የሊዝ አዋጅ ቁጥር 721 2004 እና ይህን አዋጅ ተከትለዉ የወጡት መመሪያዎች ተፈጻሚነት አላቸዉ ወይስ የላቸዉም ?\n",
            "-----------\n",
            "Score: 0.8011926\n",
            "Match: ዝርዝር አፈጻጸሙ በሕግ ይወሰናል ።\n",
            "-----------\n",
            "Score: 0.799701\n",
            "Match: የፍትህ ሥርዓት እና የሕግ ትምህርት ማሻሻያ ፕሮግራሞችን እና ሌሎች በፍትህ ዘርፉ ውስጥ ተግባራዊ የሚደረጉ ማሻሻያ ፕሮግራሞችን ያቀናጃል ፤ ያስተባብራል ፤ 4 .\n",
            "-----------\n",
            "Score: 0.7990596\n",
            "Match: ዝርዝር አፈጻጸሙ በሕግ ይወስናል ።\n",
            "-----------\n",
            "Score: 0.79869676\n",
            "Match: ፱ ፪ሺ8 የሕግ ማሻሻያና ለፍትህ ሥርዓቱ ጠቃሚነት ያላቸው ጥናቶችን እንዲሁም የኮዲፊኬሽን የፌደራል ሕጎችን የማጠቃለል ሥራ ያካሂዳል ፤ የክልሎችን ሕጎች ያሰባስባል ፣ እንደአስፈላጊነቱም እንዲጠቃለሉ ያደርጋል ፤ 3 በፌደራል መንግሥት አካላትና በክልሎች ጥያቄ ሲቀርብለት የሕግ ረቂቆች በማዘጋጀት ይረዳል ፤ 4 የወንጀል መነሻዎችን ያጠናል ፣ ወንጀል የሚቀንስበትን ሥልት ይቀይሳል ፤ በወንጀል መከላከል ረገድ የሚመለከታቸውን መንግሥታዊ አካላትና ኀብረተሰቡን ያስተባብራል ፤ 5 በፌደራል ፍርድ ቤቶች ሥልጣን ሥር የሚወድቅ ወንጀል ተፈጽሟል ብሎ ሲያምን ምርመራ ያካሂዳል ወይም እንዲካሄድ ያደርጋል ፣ የምርመራ ሂደቱን ይመራል ፣ ይቆጣጠራል ፤ የጥፋተኝነት ድርድር ይፈቅዳል ፣ ይደራደራል ፤ በቂ ምክንያት ሲኖር የተጀመረን ምርመራ ያቋርጣል ወይም ተጨማሪ ምርመራ ያከናውናል ፤ 6 የፌደራል መንግሥትን በመወከል የወንጀል ክስ ይመሠርታል ፣ ይከራከራል ፤ በቂ ምክንያት ሲኖር በሕግ መሠረት ክሱን ያነሳል ፤ ፍርድ ቤቶች የሚሰጧቸውን ውሳኔዎች መፈጸማቸውን ይከታተላል ፤ 7 የወንጀል ፍትህ መረጃዎች አሰባሰብ ፣ አያያዝና ሥርጭት ሥርዓት ይዘረጋል ፤ ለሚመለከታቸው የፍትህ አካላት ድጋፍ ይሰጣል ፤ 2 8ሺ5፻፺7 ነU 0፪ ህዳር !\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"የፋይናንስ ድጋፍ መመሪያዎች ምንድን ናቸው?\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(\"Score:\", r.score)\n",
        "    print(\"Match:\", r.payload[\"text\"])\n",
        "    print(\"-----------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq6E67Xn3IjS",
        "outputId": "4393ce12-188b-49a2-f70d-a559186cc3b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-f761defd6fd0>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.7839428\n",
            "Match: የመንግስት የፋይናንስ ድጋፍ የሚከፋፈልበት መርህ 1 .\n",
            "-----------\n",
            "Score: 0.77438617\n",
            "Match: ለዕለት ተዕለት ተግባር ማከናወኛ ስለሚሰጥ የፋይናንስ ድጋፍ 1 .\n",
            "-----------\n",
            "Score: 0.74636734\n",
            "Match: በአገልግሎት መልክ ስለሚሰጥ የመንግስት የፋይናንስ ድጋፍ 1 .\n",
            "-----------\n",
            "Score: 0.7431911\n",
            "Match: በመንግስት የሚሰጥ የፋይናንስ ድጋፍ ቦርዱ በሚያወጣው ቀመር መሰረት ይከፋፈላል ።\n",
            "-----------\n",
            "Score: 0.72906125\n",
            "Match: የተቋሙ የፋይናንስ በጀት አፈፃፀም .\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"የወንጀል ሕጉ በአስገድዶ መድፈር ወንጀል ላይ ምን ይላል?\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(\"Score:\", r.score)\n",
        "    print(\"Match:\", r.payload[\"text\"])\n",
        "    print(\"-----------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoVjgjZk4cpy",
        "outputId": "a27ce43e-a10f-44ca-9454-6583c8986539"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e4872ab9f0dd>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.8594738\n",
            "Match: የወንጀል ህግ መሰረት ይቀጣል ።\n",
            "-----------\n",
            "Score: 0.85906124\n",
            "Match: የወንጀሇኛ መቅጫ ህግ ዴንጋጌዎች እንዯተጠበቁ ሆኖ ይህን አዋጅ ወይም ላሊ አግባብ ያሇውን ህግ የሚተሊሇፌ ማንኛውም ሰው ጥፊት ፇጽሟሌና ቅጣት ይጣሌበታሌ 2 .\n",
            "-----------\n",
            "Score: 0.84386325\n",
            "Match: በወንጀል ሕግ እና በሌሎች ሕጎች የተመለከተው የወንጀል ወይም የደንብ መተላለፍ ቅጣት እንደተጠበቀ ሆኖ ባለሥልጣኑ ይህንን ደንብ በተላለፈ ማንኛውንም ሰው ላይ ሀ እንደ ጥፋቱ ሁኔታ ድርጊቱን የማስቆም ፣ የማዘጋት ፣ የማሸግ እና የማስነሳት እርምጃ ይወስዳል ፤ ለ ከዚህ ደንብ ጋር አባሪ በተደረገው ሰንጠረዥ ወይም በሌሎች አግባብ ባላቸው ሕጎች መሰረት አስተዳደራዊ ቅጣት ይቀጣል ፤ 2 .\n",
            "-----------\n",
            "Score: 0.84386325\n",
            "Match: በወንጀል ሕግ እና በሌሎች ሕጎች የተመለከተው የወንጀል ወይም የደንብ መተላለፍ ቅጣት እንደተጠበቀ ሆኖ ባለሥልጣኑ ይህንን ደንብ በተላለፈ ማንኛውንም ሰው ላይ ሀ እንደ ጥፋቱ ሁኔታ ድርጊቱን የማስቆም ፣ የማዘጋት ፣ የማሸግ እና የማስነሳት እርምጃ ይወስዳል ፤ ለ ከዚህ ደንብ ጋር አባሪ በተደረገው ሰንጠረዥ ወይም በሌሎች አግባብ ባላቸው ሕጎች መሰረት አስተዳደራዊ ቅጣት ይቀጣል ፤ 2 .\n",
            "-----------\n",
            "Score: 0.84295493\n",
            "Match: የተፈጻሚነት ወሰን ይህ አዋጅ በዚህ አዋጅ አንቀጽ ፭ ንዑስ አንቀጽ ፩ እና ፪ የተመለከቱ የወንጀል ድርጊቶችን ፈጽሟል ተብሎ ተጠርጥሮ በሚፈለግ ፣ በምርመራ ላይ ያለ ፣ የወንጀል ክስ የተመሰረተበት ወይም የጥፋተኝነት ፍርድ ውሳኔ የተላለፈበት ሰው ወይም የሽብርተኛ ድርጅት ላይ ተፈጻሚ ይሆናል ፬ .\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"ስለ መሬት ይዞታ የሕግ መረጃ ምን ይላል?\"\n",
        "query_vec = model.encode(query).tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"amharic_sentences\",\n",
        "    query_vector=query_vec,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "for r in results:\n",
        "    print(\"Score:\", r.score)\n",
        "    print(\"Match:\", r.payload[\"text\"])\n",
        "    print(\"-----------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw6ieplg4g_o",
        "outputId": "30b8362e-3467-4d74-d17b-16b611177ff3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d430e4bb2ed3>:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.81570876\n",
            "Match: በከተማ የተካሇሇ የአርሶ አዯር መሬት ይዜታ ሇማረጋገጥ የመሬት ይዜታውን ሲያስተዲዴር ከነበረው የገጠር የመሬት አስተዲዯር አካሌ የቦታውን ስፊት ፣ የባሇይዜታውን የዴርሻ መጠን ፣ በይዜታው ሊይ ያሇውን መብት ፣ ክሌከሊና ኃሊፉነት በዛርዛር ሇይቶ ሇከተማ የመሬት መጠቀም መብት ሰጪ አካሌ የሰጠው ሰነዴ መቅረብ አሇበት 2 .\n",
            "-----------\n",
            "Score: 0.80809116\n",
            "Match: በመሬት የመጠቀም መብት እንዲሰጥ ሥልጣን በተሰጠው አግባብ ባለው አካል ፈቃድ መብት ተሰጥቶ በነባር ይዞታ ሥሪት የሚተዳደር መሬት ሲሆን የመሬት ይዞታዎች የሚከተሉት መረጃዎች ተረጋግጠው መመዝገብ ይኖርባቸዋል ሀ የመሬት ይዞታው ተጠቃሚ ሙሉ ስም ፤ ለ የቦታው ስፋትና አድራሻ ፤ ሐ የቦታው የአገልግሎት ዓይነት ፣ ደረጃ ፣ የከተማ ጣቢያ እና የቁራሽ መሬት ልዩ መለያ ኮድ ፤ መ ዓመታዊ የኪራይ ክፍያ መጠን ።\n",
            "-----------\n",
            "Score: 0.80189735\n",
            "Match: የመሬት ይዞታ መብት ለማስመዝገብ የማመልከቻ አቀራረብ 1 .\n",
            "-----------\n",
            "Score: 0.7946358\n",
            "Match: በመሬት ይዞታ መብት የተረጋገጠን ሰነድ ስለማስተላለፍ 1 .\n",
            "-----------\n",
            "Score: 0.7945685\n",
            "Match: የሚከተሉት ሁኔታዎች ሲያጋጥሙ የከተማ መሬት ይዞታ ለጊዜው ሳይረጋገጥ ይቆያል ፦ ሀ በመሬት ይዞታ ማረጋገጫ ሠፈር ውስጥ የይዞታ ማረጋገጥ ስራ በህዝብ ማስታወቂያ ይፋ ከመደረጉ በፊት ሥልጣን ባለው አካል የተሰጠ የማገጃ ትዕዛዝ ካለና ይኸው ትዕዛዝ ስለመነሳቱ ማስረጃ ካልቀረበ በእገዳ ላይ ያለው የመሬት ይዞታ ለጊዜው ሳይረጋገጥ ይቆያል ፣ ሆኖም የወሰን መረጃው ይሰበስባል ፤ ለ የመሬት ይዞታ መብት ሰጪ ተቋም በላከው የይዞታ ሰነድ እና የመሬት ይዞታ ባለመብት ነኝ ባዩ ባቀረበው ሰነድ መካከል ልዩነት ሲፈጠር ፣ እንዲሁም በሰነዶቹ እና በቁራሽ መሬት ልኬት መካከል በታየው ልዩነት ላይ መብት ሰጪ ተቋሙ ውሳኔውን መግለጹ አስፈላጊ ሆኖ ጥያቄ ቀርቦለት ለመዝጋቢ ተቋሙ እስኪገልጽ ድረስ ይኽው ቁራሽ መሬት ሳይረጋገጥ የፌዴራል ጠቅላይ ዐቃቤ ህግ 842 ይቆያል ፤ በተጨማሪ ይህ ሁኔታ ለዚህ ዓላማ በተዘጋጀ የመሬት ይዞታ ክርክር መዝገብ ውስጥ እንዲመዘገብ ይደረጋል ፤ ሐ የመሬት ይዞታ ባለመብት ነኝ ባይ የባለመብትነት ማስረጃ አቅርቦ ነገር ግን የመሬት ይዞታ መብት ሰጪ ተቋም ይህን የሚያስረዳ ነባር ሰነድ ለመዝጋቢው ተቋም ያላስረከበ ከሆነ ፣ ባለመብት ነኝ ባዩ ያቀረበው ሰነድ ለመብት ሰጪ ተቋም ቀርቦ ተቋሙ ውሳኔውን ለመዝጋቢ ተቋም እስኪያሳውቅ ድረስ የመሬት ይዞታው ሳይረጋገጥ ይቆያል ፤ መ ወደ ከተማ የተጠቃለለን የገጠር መሬት በሚመለከት መሬቱን ያስተዳድር የነበረው አካል በላከው ሰነድ ላይ በተገለጸው የመሬት ስፋትና በመስክ ልኬት በተገኘው የመሬት ስፋት መካከል ልዩነት ከተገኘ ፣ በተፈጠረው የመሬት ስፋት ልዩነቱ ላይ መብት ሰጪው አካል ውሳኔ እስኪሰጥበት ድረስ የመሬት ይዞታው ሳይረጋገጥ ይቆያል ፤ 2 .\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "import json\n",
        "\n",
        "client = QdrantClient(\n",
        "    url=\"https://7a42a360-46c6-4155-a4a2-e358ec60b353.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.TK2U6DP4CyVj_W59YVOhbLDVEq07a2Y0b6Hm3PwcVhs\"\n",
        ")\n",
        "COLLECTION = \"amharic_sentences\"\n",
        "OUT_FILE = \"/content/amharic_sentences_dump.jsonl\"\n",
        "BATCH_SIZE = 500\n",
        "offset = 0\n",
        "has_more = True\n",
        "\n",
        "with open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    while has_more:\n",
        "        # Scroll through collection\n",
        "        response = client.scroll(\n",
        "            collection_name=COLLECTION,\n",
        "            offset=offset,\n",
        "            limit=BATCH_SIZE,\n",
        "            with_vectors=True,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        points = response[0]\n",
        "        has_more = len(points) == BATCH_SIZE\n",
        "        offset = response[1]\n",
        "\n",
        "        for point in points:\n",
        "            json.dump({\n",
        "                \"id\": point.id,\n",
        "                \"vector\": point.vector,\n",
        "                \"payload\": point.payload\n",
        "            }, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"✅ Exported to {OUT_FILE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b1kR95pEx6V",
        "outputId": "d96456c5-23e0-4b1d-86a7-e2abe071ba6c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Exported to /content/amharic_sentences_dump.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client google-generativeai sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rTuQ2mjr4ymX",
        "outputId": "c70fb89a-1b4d-4646-f309-279489a3a495"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.11/dist-packages (1.14.2)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.0.2)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (5.29.4)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.4.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "f77e2d725c4145a29a74faabb6a10b4d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyA14gFgZIkfIP76yfn5nz9NzJ8LR9VMzGw\")\n",
        "\n",
        "models = list(genai.list_models())  # convert generator to list\n",
        "\n",
        "for model in models:\n",
        "    print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xpLq8WKyKtZe",
        "outputId": "c31a5e4c-8e6b-4ede-ddf8-5debbc01123a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
            "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
            "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.5-pro-exp-03-25',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.5 Pro Experimental 03-25',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-03-25',\n",
            "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
            "      description='Gemini 2.5 Pro Preview 03-25',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-04-17',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-04-17',\n",
            "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-04-17-thinking',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-04-17',\n",
            "      display_name='Gemini 2.5 Flash Preview 04-17 for cursor testing',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-06',\n",
            "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
            "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite 001',\n",
            "      description='Stable version of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite',\n",
            "      description='Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Preview Image Generation',\n",
            "      description='Gemini 2.0 Flash Preview Image Generation',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-pro-exp',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.0 Pro Experimental',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-04-17',\n",
            "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-04-17',\n",
            "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-04-17',\n",
            "      display_name='Gemini 2.5 Flash Preview 04-17',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/learnlm-2.0-flash-experimental',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='LearnLM 2.0 Flash Experimental',\n",
            "      description='LearnLM 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-1b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 1B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 4B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-12b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 12B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-27b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 27B',\n",
            "      description='',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-exp-03-07',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental 03-07',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-exp',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n",
            "Model(name='models/imagen-3.0-generate-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Imagen 3.0 002 model',\n",
            "      description='Vertex served Imagen 3.0 002 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-2.0-flash-live-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description='Gemini 2.0 Flash 001',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ]
    }
  ]
}